{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzMsr9fh0xPG",
        "outputId": "d44956e0-b53d-48e2-8622-7c7110188019"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Convertido caso_6.txt para caso_6.feature\n",
            "Convertido caso_7.txt para caso_7.feature\n",
            "Convertido caso_2.txt para caso_2.feature\n",
            "Convertido caso_3.txt para caso_3.feature\n",
            "Convertido caso_9.txt para caso_9.feature\n",
            "Convertido caso_1.txt para caso_1.feature\n",
            "Convertido caso_10.txt para caso_10.feature\n",
            "Convertido caso_4.txt para caso_4.feature\n",
            "Convertido caso_5.txt para caso_5.feature\n",
            "Convertido caso_8.txt para caso_8.feature\n",
            "Resultados salvos em ranking_similaridade_meteor_gpt-3.5-turbo-0125_few_shot.csv\n",
            "\n",
            "Visualização do DataFrame com o Ranking de Similaridade:\n",
            "          Caso Base Execução Comparada  Pontuação METEOR  Ranking\n",
            "0    caso_1.feature             exec_4              0.81        1\n",
            "1    caso_1.feature             exec_1              0.79        2\n",
            "2    caso_1.feature             exec_3              0.79        3\n",
            "3    caso_1.feature             exec_2              0.75        4\n",
            "4    caso_1.feature             exec_5              0.72        5\n",
            "5    caso_2.feature             exec_5              0.75        1\n",
            "6    caso_2.feature             exec_4              0.67        2\n",
            "7    caso_2.feature             exec_1              0.65        3\n",
            "8    caso_2.feature             exec_3              0.64        4\n",
            "9    caso_2.feature             exec_2              0.63        5\n",
            "10   caso_3.feature             exec_3              0.89        1\n",
            "11   caso_3.feature             exec_4              0.89        2\n",
            "12   caso_3.feature             exec_5              0.89        3\n",
            "13   caso_3.feature             exec_2              0.70        4\n",
            "14   caso_3.feature             exec_1              0.59        5\n",
            "15   caso_4.feature             exec_1              0.82        1\n",
            "16   caso_4.feature             exec_2              0.81        2\n",
            "17   caso_4.feature             exec_3              0.81        3\n",
            "18   caso_4.feature             exec_4              0.81        4\n",
            "19   caso_4.feature             exec_5              0.76        5\n",
            "20   caso_5.feature             exec_2              0.68        1\n",
            "21   caso_5.feature             exec_4              0.68        2\n",
            "22   caso_5.feature             exec_5              0.68        3\n",
            "23   caso_5.feature             exec_1              0.66        4\n",
            "24   caso_5.feature             exec_3              0.56        5\n",
            "25   caso_6.feature             exec_5              0.85        1\n",
            "26   caso_6.feature             exec_4              0.84        2\n",
            "27   caso_6.feature             exec_3              0.74        3\n",
            "28   caso_6.feature             exec_1              0.72        4\n",
            "29   caso_6.feature             exec_2              0.72        5\n",
            "30   caso_7.feature             exec_1              0.89        1\n",
            "31   caso_7.feature             exec_2              0.89        2\n",
            "32   caso_7.feature             exec_3              0.89        3\n",
            "33   caso_7.feature             exec_4              0.89        4\n",
            "34   caso_7.feature             exec_5              0.89        5\n",
            "35   caso_8.feature             exec_2              0.71        1\n",
            "36   caso_8.feature             exec_1              0.66        2\n",
            "37   caso_8.feature             exec_5              0.64        3\n",
            "38   caso_8.feature             exec_3              0.61        4\n",
            "39   caso_8.feature             exec_4              0.61        5\n",
            "40   caso_9.feature             exec_1              0.77        1\n",
            "41   caso_9.feature             exec_2              0.74        2\n",
            "42   caso_9.feature             exec_5              0.74        3\n",
            "43   caso_9.feature             exec_3              0.72        4\n",
            "44   caso_9.feature             exec_4              0.70        5\n",
            "45  caso_10.feature             exec_4              0.77        1\n",
            "46  caso_10.feature             exec_5              0.77        2\n",
            "47  caso_10.feature             exec_2              0.70        3\n",
            "48  caso_10.feature             exec_3              0.70        4\n",
            "49  caso_10.feature             exec_1              0.69        5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from itertools import chain\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def find_zip_file(prefix):\n",
        "    for file in os.listdir():\n",
        "        if file.startswith(prefix) and file.endswith('.zip'):\n",
        "            return file\n",
        "    raise FileNotFoundError(f\"Nenhum arquivo ZIP encontrado com o prefixo '{prefix}'.\")\n",
        "\n",
        "def unzip_and_remove(zip_file_path):\n",
        "    base_dir = zip_file_path.replace(\".zip\", \"\")\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(base_dir)\n",
        "\n",
        "    os.remove(zip_file_path)\n",
        "\n",
        "    nested_dir = os.path.join(base_dir, os.path.basename(base_dir))\n",
        "    if os.path.exists(nested_dir):\n",
        "        for file_name in os.listdir(nested_dir):\n",
        "            os.rename(os.path.join(nested_dir, file_name), os.path.join(base_dir, file_name))\n",
        "        os.rmdir(nested_dir)\n",
        "\n",
        "    return base_dir\n",
        "\n",
        "def convert_ground_truth_to_feature(ground_truth_dir):\n",
        "    for file_name in os.listdir(ground_truth_dir):\n",
        "        if file_name.startswith(\"caso_\") and not file_name.endswith(\".feature\"):\n",
        "            original_path = os.path.join(ground_truth_dir, file_name)\n",
        "\n",
        "            file_base_name, file_extension = os.path.splitext(file_name)\n",
        "            if file_extension: \n",
        "                new_file_name = f\"{file_base_name}.feature\"\n",
        "            else:  \n",
        "                new_file_name = f\"{file_name}.feature\"\n",
        "\n",
        "            new_path = os.path.join(ground_truth_dir, new_file_name)\n",
        "            os.rename(original_path, new_path)\n",
        "            print(f\"Convertido {file_name} para {new_file_name}\")\n",
        "\n",
        "def read_feature_files(directory):\n",
        "    feature_files = {}\n",
        "    for file_name in os.listdir(directory):\n",
        "        if file_name.endswith(\".feature\"):\n",
        "            with open(os.path.join(directory, file_name), 'r') as file:\n",
        "                feature_files[file_name] = file.read()\n",
        "    return feature_files\n",
        "\n",
        "def expand_with_synonyms(tokens):\n",
        "    expanded_tokens = []\n",
        "    for token in tokens:\n",
        "        expanded_tokens.append(token)\n",
        "        synonyms = wordnet.synsets(token)\n",
        "        lemmas = set(chain.from_iterable([syn.lemma_names() for syn in synonyms]))\n",
        "        expanded_tokens.extend(lemmas)\n",
        "    return expanded_tokens\n",
        "\n",
        "def calculate_meteor_with_synonyms(text1, text2):\n",
        "    tokens1 = word_tokenize(text1.lower())\n",
        "    tokens2 = word_tokenize(text2.lower())\n",
        "\n",
        "    expanded_tokens1 = expand_with_synonyms(tokens1)\n",
        "    expanded_tokens2 = expand_with_synonyms(tokens2)\n",
        "\n",
        "    score = single_meteor_score(expanded_tokens2, expanded_tokens1)\n",
        "    return round(score, 2)\n",
        "\n",
        "def read_model_info(base_dir):\n",
        "    model_info_path = os.path.join(base_dir, \"model.txt\")\n",
        "    with open(model_info_path, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "    model = lines[0].split(\":\")[1].strip()\n",
        "    technique = lines[1].split(\":\")[1].strip()\n",
        "    return model, technique\n",
        "\n",
        "def compare_with_ground_truth(ground_truth_dir, features_dir):\n",
        "    results = []\n",
        "\n",
        "    ground_truth_files = read_feature_files(ground_truth_dir)\n",
        "\n",
        "    for i in range(1, 11):  \n",
        "        ground_truth_file_name = f\"caso_{i}.feature\"\n",
        "        if ground_truth_file_name not in ground_truth_files:\n",
        "            continue\n",
        "\n",
        "        ground_truth_content = ground_truth_files[ground_truth_file_name]\n",
        "        scores = []\n",
        "\n",
        "        for exec_num in range(1, 6):\n",
        "            compare_files = read_feature_files(os.path.join(features_dir, f\"exec_{exec_num}\"))\n",
        "            if ground_truth_file_name in compare_files:\n",
        "                compare_content = compare_files[ground_truth_file_name]\n",
        "                score = calculate_meteor_with_synonyms(ground_truth_content, compare_content)\n",
        "                scores.append((f\"exec_{exec_num}\", score))\n",
        "\n",
        "        scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        for rank, (exec_name, score) in enumerate(scores, 1):\n",
        "            results.append({\"Caso Base\": ground_truth_file_name, \"Execução Comparada\": exec_name, \"Pontuação METEOR\": score, \"Ranking\": rank})\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def main():\n",
        "    ground_truth_zip = find_zip_file('ground_truth')\n",
        "    features_zip = find_zip_file('features-')\n",
        "\n",
        "    ground_truth_dir = unzip_and_remove(ground_truth_zip)\n",
        "    features_dir = unzip_and_remove(features_zip)\n",
        "\n",
        "    convert_ground_truth_to_feature(ground_truth_dir)\n",
        "\n",
        "    model, technique = read_model_info(features_dir)\n",
        "\n",
        "    df_results = compare_with_ground_truth(ground_truth_dir, features_dir)\n",
        "\n",
        "    model_safe = model.replace(\"/\", \"-\")\n",
        "    technique_safe = technique.replace(\"/\", \"-\")\n",
        "\n",
        "    output_csv = f\"ranking_similaridade_meteor_{model_safe}_{technique_safe}.csv\"\n",
        "\n",
        "    df_results.to_csv(output_csv, index=False)\n",
        "    print(f\"Resultados salvos em {output_csv}\")\n",
        "\n",
        "    print(\"\\nVisualização do DataFrame com o Ranking de Similaridade:\")\n",
        "    print(df_results)\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USf7hh90UVKe",
        "outputId": "bef93258-3798-4242-ee6d-22be9200ff7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diretório removido: /content/ground_truth\n",
            "Diretório removido: /content/features-gpt-3.5-turbo-0125\n",
            "Arquivo CSV removido: /content/ranking_similaridade_meteor_gpt-3.5-turbo-0125_few_shot.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def remove_unzipped_directories_and_csv():\n",
        "    \"\"\"\n",
        "    Remove diretórios que começam com 'features-', o diretório 'ground_truth' e arquivos .csv.\n",
        "    \"\"\"\n",
        "    current_dir = os.getcwd()\n",
        "    directories = [d for d in os.listdir(current_dir) if os.path.isdir(os.path.join(current_dir, d))]\n",
        "\n",
        "    for directory in directories:\n",
        "        if directory.startswith('features-') or directory == 'ground_truth':\n",
        "            dir_path = os.path.join(current_dir, directory)\n",
        "            try:\n",
        "                shutil.rmtree(dir_path)  \n",
        "                print(f\"Diretório removido: {dir_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao remover {dir_path}: {e}\")\n",
        "\n",
        "    csv_files = [f for f in os.listdir(current_dir) if f.endswith('.csv')]\n",
        "\n",
        "    for file_name in csv_files:\n",
        "        file_path = os.path.join(current_dir, file_name)\n",
        "        try:\n",
        "            os.remove(file_path)  \n",
        "            print(f\"Arquivo CSV removido: {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao remover {file_path}: {e}\")\n",
        "\n",
        "remove_unzipped_directories_and_csv()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
